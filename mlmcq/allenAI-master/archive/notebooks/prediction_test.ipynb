{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import numpy as np\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import recurrent\n",
    "from keras.models import Graph, Sequential\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import cPickle as pickle\n",
    "import theano.tensor as T\n",
    "from theano import function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean(x, axis=None, keepdims=False):\n",
    "    return T.mean(x, axis=axis, keepdims=keepdims)\n",
    "\n",
    "def l2_normalize(x, axis):\n",
    "    norm = T.sqrt(T.sum(T.square(x), axis=axis, keepdims=True))\n",
    "    return x / norm\n",
    "\n",
    "def cosine_similarity(y_true, y_pred):\n",
    "    assert y_true.ndim == 2\n",
    "    assert y_pred.ndim == 2\n",
    "    y_true = l2_normalize(y_true, axis=1)\n",
    "    y_pred = l2_normalize(y_pred, axis=1)\n",
    "    return T.sum(y_true * y_pred, axis=1, keepdims=False)\n",
    "\n",
    "def cosine_ranking_loss(y_true, y_pred):\n",
    "    q = y_pred[0::3]\n",
    "    a_correct = y_pred[1::3]\n",
    "    a_incorrect = y_pred[2::3]\n",
    "\n",
    "    return mean(T.maximum(0., args.margin - cosine_similarity(q, a_correct) + cosine_similarity(q, a_incorrect)) - y_true[0]*0, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def np_l2_normalize(x, axis):\n",
    "    norm = np.sqrt(np.sum(np.square(x), axis=axis, keepdims=True))\n",
    "    return x / norm\n",
    "\n",
    "def np_cosine_similarity(y_true, y_pred):\n",
    "    assert y_true.ndim == 2\n",
    "    assert y_pred.ndim == 2\n",
    "    y_true = np_l2_normalize(y_true, axis=1)\n",
    "    y_pred = np_l2_normalize(y_pred, axis=1)\n",
    "    return np.sum(y_true * y_pred, axis=1, keepdims=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"model_path\")\n",
    "parser.add_argument(\"csv_file\")\n",
    "parser.add_argument(\"--write_predictions\", default=\"predictions.csv\")\n",
    "parser.add_argument(\"--tokenizer\", default=\"model/tokenizer.pkl\")\n",
    "parser.add_argument(\"--rnn\", choices=[\"LSTM\", \"GRU\"], default=\"GRU\")\n",
    "parser.add_argument(\"--embed_size\", type=int, default=300)\n",
    "parser.add_argument(\"--hidden_size\", type=int, default=1024)\n",
    "parser.add_argument(\"--layers\", type=int, default=1)\n",
    "parser.add_argument(\"--dropout\", type=float, default=0)\n",
    "parser.add_argument(\"--bidirectional\", action='store_true', default=False)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=300)\n",
    "parser.add_argument(\"--maxlen\", type=int)\n",
    "parser.add_argument(\"--vocab_size\", type=int)\n",
    "parser.add_argument(\"--optimizer\", choices=['adam', 'rmsprop'], default='adam')\n",
    "parser.add_argument(\"--verbose\", type=int, choices=[0, 1, 2], default=1)\n",
    "parser.add_argument(\"--margin\", type=float, default=0.01)\n",
    "#args = parser.parse_args(\"model/simple_1400000.pkl data/training_set.tsv --vocab_size 107149\".split())\n",
    "#args = parser.parse_args(\"model/simple_reduced.hdf5 data/training_set.tsv --tokenizer model/tokenizer_reduced.pkl\".split())\n",
    "#args = parser.parse_args(\"model/simple_bidirectional.hdf5 data/training_set.tsv --vocab_size 107149 --bidirectional\".split())\n",
    "args = parser.parse_args(\"model/simple_margin_0.1.pkl data/training_set.tsv --vocab_size 107149\".split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Questions:  2500\n"
     ]
    }
   ],
   "source": [
    "print \"Loading data...\"\n",
    "ids = []\n",
    "questions = []\n",
    "corrects = []\n",
    "answersA = []\n",
    "answersB = []\n",
    "answersC = []\n",
    "answersD = []\n",
    "with open(args.csv_file) as f:\n",
    "  reader = csv.reader(f, delimiter=\"\\t\", strict=True, quoting=csv.QUOTE_NONE)\n",
    "  line = next(reader)  # ignore header\n",
    "  is_train_set = (len(line) == 7)\n",
    "  for line in reader:\n",
    "    ids.append(line[0])\n",
    "    questions.append(line[1])\n",
    "    if is_train_set:\n",
    "      corrects.append(line[2])\n",
    "      answersA.append(line[3])\n",
    "      answersB.append(line[4])\n",
    "      answersC.append(line[5])\n",
    "      answersD.append(line[6])\n",
    "    else:\n",
    "      answersA.append(line[2])\n",
    "      answersB.append(line[3])\n",
    "      answersC.append(line[4])\n",
    "      answersD.append(line[5])\n",
    "print \"Questions: \", len(questions)\n",
    "assert len(questions) == len(answersA) == len(answersB) == len(answersC) == len(answersD)\n",
    "assert not is_train_set or len(corrects) == len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample question and answers:\n",
      "When athletes begin to exercise, their heart rates and respiration rates increase.  At what level of organization does the human body coordinate these functions? A: at the tissue level B: at the organ level C: at the system level D: at the cellular level Correct:  C\n",
      "Which example describes a learned behavior in a dog? A: smelling the air for odors B: barking when disturbed C: sitting on command D: digging in soil Correct:  C\n",
      "When two nuclei are combined into one nucleus, there is a slight change in mass and the release of a large amount of energy. What is this process called? A: conversion B: reaction C: fission D: fusion Correct:  D\n"
     ]
    }
   ],
   "source": [
    "print \"Sample question and answers:\"\n",
    "for i in xrange(3):\n",
    "  print questions[i], \"A:\", answersA[i], \"B:\", answersB[i], \"C:\", answersC[i], \"D:\", answersD[i], \"Correct: \", corrects[i] if is_train_set else '?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts size: 12500\n"
     ]
    }
   ],
   "source": [
    "texts = questions + answersA + answersB + answersC + answersD\n",
    "print \"Texts size:\", len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = pickle.load(open(args.tokenizer, \"rb\"))\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences maxlen: 179\n"
     ]
    }
   ],
   "source": [
    "if args.maxlen:\n",
    "  maxlen = args.maxlen\n",
    "else:\n",
    "  maxlen = max([len(s) for s in sequences])\n",
    "print \"Sequences maxlen:\", maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = pad_sequences(sequences, maxlen=maxlen) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding original vocabulary size 107149\n",
      "Vocabulary size: 107149 Texts:  (12500, 179)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.nb_words if tokenizer.nb_words else len(tokenizer.word_index)+1\n",
    "if args.vocab_size:\n",
    "  print \"Overriding original vocabulary size\", vocab_size\n",
    "  vocab_size = args.vocab_size\n",
    "print \"Vocabulary size:\", vocab_size, \"Texts: \", texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if args.rnn == 'GRU':\n",
    "  RNN = recurrent.GRU\n",
    "elif args.rnn == 'LSTM':\n",
    "  RNN = recurrent.LSTM\n",
    "else:\n",
    "  assert False, \"Invalid RNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "--------------------------------------------------------------------------------\n",
      "Initial input shape: (None, 107149)\n",
      "--------------------------------------------------------------------------------\n",
      "Layer (name)                  Output Shape                  Param #             \n",
      "--------------------------------------------------------------------------------\n",
      "Embedding (embedding)         (None, None, 300)             32144700            \n",
      "GRU (gru)                     (None, 1024)                  4070400             \n",
      "--------------------------------------------------------------------------------\n",
      "Total params: 36215100\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print \"Creating model...\"\n",
    "\n",
    "if args.bidirectional:\n",
    "  model = Graph()\n",
    "  model.add_input(name=\"input\", batch_input_shape=(args.batch_size,)+texts.shape[1:], dtype=\"uint\")\n",
    "  model.add_node(Embedding(vocab_size, args.embed_size, mask_zero=True), name=\"embed\", input='input')\n",
    "  for i in xrange(args.layers):\n",
    "    model.add_node(RNN(args.hidden_size, return_sequences=False if i + 1 == args.layers else True), \n",
    "        name='forward'+str(i+1), \n",
    "        input='embed' if i == 0 else 'dropout'+str(i) if args.dropout > 0 else None, \n",
    "        inputs=['forward'+str(i), 'backward'+str(i)] if i > 0 and args.dropout == 0 else [])\n",
    "    model.add_node(RNN(args.hidden_size, return_sequences=False if i + 1 == args.layers else True, go_backwards=True), \n",
    "        name='backward'+str(i+1), \n",
    "        input='embed' if i == 0 else 'dropout'+str(i) if args.dropout > 0 else None, \n",
    "        inputs=['forward'+str(i), 'backward'+str(i)] if i > 0 and args.dropout == 0 else [])\n",
    "    if args.dropout > 0:\n",
    "      model.add_node(Dropout(args.dropout), name='dropout'+str(i+1), inputs=['forward'+str(i+1), 'backward'+str(i+1)])\n",
    "  model.add_output(name='output',\n",
    "      input='dropout'+str(args.layers) if args.dropout > 0 else None,\n",
    "      inputs=['forward'+str(args.layers), 'backward'+str(args.layers)] if args.dropout == 0 else [])\n",
    "else:\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(vocab_size, args.embed_size, mask_zero=True))\n",
    "  for i in xrange(args.layers):\n",
    "    model.add(RNN(args.hidden_size, return_sequences=False if i + 1 == args.layers else True))\n",
    "    if args.dropout > 0:\n",
    "      model.add(Dropout(args.dropout))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from model/simple_margin_0.1.pkl\n"
     ]
    }
   ],
   "source": [
    "print \"Loading weights from %s\" % args.model_path\n",
    "model.load_weights(args.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...\n"
     ]
    }
   ],
   "source": [
    "print \"Compiling model...\"\n",
    "if args.bidirectional:\n",
    "  model.compile(optimizer=args.optimizer, loss={'output': cosine_ranking_loss})\n",
    "else:\n",
    "  model.compile(optimizer=args.optimizer, loss=cosine_ranking_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500/12500 [==============================] - 37s    \n",
      "Predictions:  (12500, 1024)\n"
     ]
    }
   ],
   "source": [
    "if args.bidirectional:\n",
    "  pred = model.predict({'input': texts}, batch_size=args.batch_size, verbose=args.verbose)\n",
    "  pred = pred['output']\n",
    "else:\n",
    "  pred = model.predict(texts, batch_size=args.batch_size, verbose=args.verbose)\n",
    "\n",
    "print \"Predictions: \", pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.25983004e-02   9.99998748e-01   7.18231720e-04   9.99998808e-01\n",
      "   9.99973297e-01   9.54918284e-03   6.92471396e-03   5.97136654e-02\n",
      "   9.53765333e-01   9.71550703e-01]\n",
      "[  1.03059423e-03   9.99998748e-01   1.34690187e-03   7.03501690e-04\n",
      "   9.98515308e-01   3.50946118e-03   2.06653844e-03   4.07464649e-05\n",
      "   1.53134271e-04   1.77163437e-01]\n",
      "[  7.73918640e-04   9.99998748e-01   2.68034782e-04   1.00000000e+00\n",
      "   9.99997973e-01   3.82178766e-03   7.41665135e-04   9.82717514e-01\n",
      "   4.83897060e-01   4.94618714e-02]\n",
      "[  7.77362438e-04   9.99998748e-01   2.30792630e-03   2.35250286e-06\n",
      "   9.99507427e-01   3.41042131e-01   7.54320179e-04   5.92470288e-01\n",
      "   1.68929546e-04   9.72303152e-01]\n",
      "[ 0.01265316  0.99999875  0.0024676   1.          0.99997342  0.00508639\n",
      "  0.01209639  0.65023321  0.95768094  0.85449421]\n"
     ]
    }
   ],
   "source": [
    "print pred[0,:10]\n",
    "print pred[1,:10]\n",
    "print pred[2,:10]\n",
    "print pred[3,:10]\n",
    "print pred[4,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 1024) (2500, 1024) (2500, 1024) (2500, 1024) (2500, 1024)\n"
     ]
    }
   ],
   "source": [
    "questions = pred[0::5]\n",
    "answersA = pred[1::5]\n",
    "answersB = pred[2::5]\n",
    "answersC = pred[3::5]\n",
    "answersD = pred[4::5]\n",
    "print questions.shape, answersA.shape, answersB.shape, answersC.shape, answersD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2500)\n",
      "[ 0.86860622  0.91262216  0.85996873  0.93499566]\n",
      "[ 0.91710509  0.90115706  0.90401681  0.89186482]\n",
      "[ 0.91669979  0.89054291  0.93537557  0.92411741]\n"
     ]
    }
   ],
   "source": [
    "sims = np.array([\n",
    "  np_cosine_similarity(questions, answersA),\n",
    "  np_cosine_similarity(questions, answersB),\n",
    "  np_cosine_similarity(questions, answersC),\n",
    "  np_cosine_similarity(questions, answersD)\n",
    "])\n",
    "print sims.shape\n",
    "print sims[:,0]\n",
    "print sims[:,1]\n",
    "print sims[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500,)\n",
      "[3 0 2]\n"
     ]
    }
   ],
   "source": [
    "preds = np.argmax(sims, axis=0)\n",
    "print preds.shape\n",
    "print preds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D', 'A', 'C']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = [chr(ord('A') + p) for p in preds]\n",
    "preds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 631 Total: 2500 Accuracy: 0.252400\n"
     ]
    }
   ],
   "source": [
    "if is_train_set:\n",
    "  correct = sum([corrects[i] == p for i,p in enumerate(preds)])\n",
    "  print \"Correct: %d Total: %d Accuracy: %f\" % (correct, len(preds), float(correct) / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
